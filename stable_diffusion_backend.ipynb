{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgileDevArt/stable-diffusion-plugins/blob/main/stable_diffusion_backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "## Stable Diffusion Backend\n",
        "\n",
        "Original Notebook by [blueturtleai](https://github.com/blueturtleai/gimp-stable-diffusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3poIEhV1oA4z"
      },
      "source": [
        "By using this Notebook, you agree to the following Terms of Use, and license:\n",
        "\n",
        "**Stablity.AI Model Terms of Use**\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "You can't use the model to deliberately produce nor share illegal or harmful outputs or content.\n",
        "CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license.\n",
        "You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-f7cQmf2Nt"
      },
      "outputs": [],
      "source": [
        "#@title NVIDIA GPU\n",
        "import subprocess\n",
        "sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(sub_p_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsOTioAILxjx"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive # type: ignore\n",
        "\n",
        "try:\n",
        "   drive_path = \"/content/drive\"\n",
        "   drive.mount(drive_path,force_remount=False)\n",
        "except:\n",
        "   print(\"...error mounting drive or with drive path variables\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TxIOPT0G5Lx1"
      },
      "outputs": [],
      "source": [
        "#@title Set Model Path\n",
        "#@markdown **Hints**\n",
        "#@markdown - It is recommended to use the default path. That way you don't have to adjust the path manually every time. \n",
        "#@markdown - If the model file doesn't exist at this location, it is automatically downloaded from Huggingface. Please make sure you have enough free space on your drive (about 4 GB).\n",
        "#@markdown - **For an individual path:**\n",
        "#@markdown  - Click on the folder symbol on the left. Open the \"drive/MyDrive\" folder and navigate to the model file which you uploaded before. Select the model file, click on the three dots and select \"copy path\". Close the file explorer via the cross. \n",
        "#@markdown  - Insert the copied path into the field. Remove the filename and the last \"/\" at the end. The path should now look for example like this ```/content/drive/MyDrive/AI/models```.\n",
        "\n",
        "import os\n",
        "\n",
        "# ask for the link\n",
        "print(\"Local Path Variables:\\n\")\n",
        "\n",
        "models_path = \"/content/models\"\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "models_path_gdrive = \"/content/drive/MyDrive/AI/models/\" #@param {type:\"string\"}\n",
        "output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\"\n",
        "models_path = models_path_gdrive\n",
        "output_path = output_path_gdrive\n",
        "\n",
        "os.makedirs(models_path, exist_ok=True)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRNl2mfepEIe"
      },
      "outputs": [],
      "source": [
        "#@title Setup Environment\n",
        "\n",
        "setup_environment = True\n",
        "print_subprocess = True\n",
        "\n",
        "if setup_environment:\n",
        "    import subprocess, time\n",
        "    print(\"Setting up environment...\")\n",
        "    start_time = time.time()\n",
        "    all_process = [\n",
        "        ['pip', 'install', 'torch==1.12.1+cu113', 'torchvision==0.13.1+cu113', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'],\n",
        "        ['pip', 'install', 'omegaconf==2.2.3', 'einops==0.4.1', 'pytorch-lightning==1.7.4', 'torchmetrics==0.9.3', 'torchtext==0.13.1', 'transformers==4.21.2', 'kornia==0.6.7', 'diffusers', 'xformers', 'triton==2.0.0.dev20221120'],\n",
        "        ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq'],\n",
        "        ['pip', 'install', 'pyngrok', 'flask-cloudflared'],\n",
        "        ['git', 'lfs', 'install'],\n",
        "    ]\n",
        "    for process in all_process:\n",
        "        running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if print_subprocess:\n",
        "            print(running)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"Environment set up in {end_time-start_time:.0f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIUJ7lWI4v53"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "from diffusers import StableDiffusionUpscalePipeline\n",
        "\n",
        "#@title **Select and Load Model**\n",
        "#@markdown **Hints**\n",
        "\n",
        "#@markdown If the model file doesn't exist at the location you selected above, it will automatically be downloaded from Huggingface. \n",
        "#@markdown Please make sure you have enough free space on your drive (about 4 GB). It is necessary, that you created an account on Huggingface and accepted the terms of service. Otherwise it's not possible to download the file. For the download you need a Huggingface token:\n",
        "#@markdown  - Login on Huggingface and select \"Settings/Access Tokens\" on the left.\n",
        "#@markdown  - Click on \"New Token\", enter a name, select \"Read\" as role, click on create and copy the token.\n",
        "model_checkpoint =  \"stable-diffusion-2-base\" #@param [\"stable-diffusion-2-base\", \"stable-diffusion-2\", \"stable-diffusion-2-inpainting\", \"stable-diffusion-x4-upscaler\", \"stable-diffusion-inpainting\", \"stable-diffusion-v1-5\"]\n",
        "\n",
        "half_precision = False\n",
        "\n",
        "model_map = {\n",
        "    \"stable-diffusion-2-base\": {\n",
        "        'url': 'https://huggingface.co/stabilityai/stable-diffusion-2-base',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"stable-diffusion-2\": {\n",
        "        'url': 'https://huggingface.co/stabilityai/stable-diffusion-2',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"stable-diffusion-2-inpainting\": {\n",
        "        'url': 'https://huggingface.co/stabilityai/stable-diffusion-2-inpainting',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"stable-diffusion-x4-upscaler\": {\n",
        "        'url': 'https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"stable-diffusion-inpainting\": {\n",
        "        'url': 'https://huggingface.co/runwayml/stable-diffusion-inpainting',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"stable-diffusion-v1-5\": {\n",
        "        'url': 'https://huggingface.co/runwayml/stable-diffusion-v1-5',\n",
        "        'requires_login': True,\n",
        "        }\n",
        "}\n",
        "\n",
        "# checkpoint path or download\n",
        "ckpt_path = os.path.join(models_path, model_checkpoint)\n",
        "if os.path.exists(ckpt_path):\n",
        "   print(f\"{ckpt_path} exists...updating...\")\n",
        "   #running = subprocess.run(['git', '-C', ckpt_path, 'reset', '--hard'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "   print(running)\n",
        "elif 'url' in model_map[model_checkpoint]:\n",
        "    url = model_map[model_checkpoint]['url']\n",
        "\n",
        "    # CLI dialogue to authenticate download\n",
        "    if model_map[model_checkpoint]['requires_login']:\n",
        "        print(\"This model requires an authentication token\")\n",
        "        print(\"Please ensure you have accepted its terms of service before continuing.\\n\")\n",
        "        print(\"Press enter after you inserted your username\")\n",
        "\n",
        "        username = input(\"What is your huggingface username?:\")\n",
        "        print(\"\\n\")\n",
        "        print(\"Press enter after you inserted your token\")\n",
        "        token = input(\"What is your huggingface token?:\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        _, path = url.split(\"https://\")\n",
        "\n",
        "        url = f\"https://{username}:{token}@{path}\"\n",
        "\n",
        "    # contact server for model\n",
        "    print(f\"Attempting to download {model_checkpoint}...this may take a while\")\n",
        "    running = None\n",
        "    if half_precision:\n",
        "       running = subprocess.run(['git', '-C', models_path, 'clone', '-b', 'fp16', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "       running = subprocess.run(['git', '-C', models_path, 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(running)\n",
        "else:\n",
        "    raise Exception(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n",
        "\n",
        "print(f\"Using model: {ckpt_path}\")\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "torch_dtype = torch.float16 if half_precision else torch.float32\n",
        "if torch.cuda.is_available():\n",
        "   torch.backends.cudnn.benchmark = True\n",
        "   torch.backends.cuda.matmul.allow_tf32 = True\n",
        "print(f\"Using torch_dtype {torch_dtype} with {device}\")\n",
        "\n",
        "text2img = None\n",
        "img2img = None\n",
        "inpaint = None\n",
        "upscale = None\n",
        "\n",
        "if model_checkpoint.endswith('upscaler'):\n",
        "   upscale = StableDiffusionUpscalePipeline.from_pretrained(ckpt_path, torch_dtype=torch_dtype)\n",
        "elif model_checkpoint.endswith('inpainting'):\n",
        "   inpaint = StableDiffusionInpaintPipeline.from_pretrained(ckpt_path, torch_dtype=torch_dtype)\n",
        "elif model_checkpoint.startswith('stable-diffusion-2'):\n",
        "   text2img = DiffusionPipeline.from_pretrained(ckpt_path, torch_dtype=torch_dtype)\n",
        "else: # sd_v1\n",
        "   text2img = StableDiffusionPipeline.from_pretrained(ckpt_path, torch_dtype=torch_dtype)\n",
        "   img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
        "\n",
        "if text2img is not None:\n",
        "   text2img = text2img.to(device_type)  \n",
        "   if torch.cuda.is_available():\n",
        "      text2img.enable_xformers_memory_efficient_attention()\n",
        "      text2img.enable_attention_slicing()\n",
        "if img2img is not None:\n",
        "   img2img = img2img.to(device_type) \n",
        "   if torch.cuda.is_available():\n",
        "      img2img.enable_xformers_memory_efficient_attention()\n",
        "      img2img.enable_attention_slicing()\n",
        "if inpaint is not None:\n",
        "   inpaint = inpaint.to(device_type) \n",
        "   if torch.cuda.is_available():\n",
        "      inpaint.enable_xformers_memory_efficient_attention()\n",
        "      inpaint.enable_attention_slicing()\n",
        "if upscale is not None:\n",
        "   upscale = upscale.to(device_type) \n",
        "   if torch.cuda.is_available():\n",
        "      upscale.enable_xformers_memory_efficient_attention()\n",
        "      upscale.enable_attention_slicing()\n",
        "\n",
        "def get_latents(seed, height, width):\n",
        "    generator = torch.Generator(device=device)\n",
        "    generator.manual_seed(seed)\n",
        "    return torch.randn(\n",
        "            (1, 4, height, width),\n",
        "            generator = generator,\n",
        "            device = device)\n",
        "\n",
        "def render_image(pipe, prompt, init_image, mask_image, strength, steps, scale, height, width, seed):\n",
        "   with torch.inference_mode():\n",
        "      # with autocast(device_type):\n",
        "         if pipe == text2img:\n",
        "            latents = get_latents(seed, height // 8, width // 8)\n",
        "            return pipe(prompt, \n",
        "                      #  init_image,\n",
        "                      #  mask_image=mask_image, \n",
        "                      #  strength=strength, \n",
        "                        guidance_scale=scale,\n",
        "                        num_inference_steps=steps,\n",
        "                        latents=latents).images  \n",
        "\n",
        "         if pipe == img2img:\n",
        "            latents = get_latents(seed, height // 8, width // 8)\n",
        "            return pipe(prompt, \n",
        "                        init_image,\n",
        "                      #  mask_image=mask_image, \n",
        "                        strength=strength, \n",
        "                        guidance_scale=scale,\n",
        "                        num_inference_steps=steps,\n",
        "                        latents=latents).images\n",
        "\n",
        "         if pipe == inpaint:\n",
        "            latents = get_latents(seed, height // 8, width // 8)\n",
        "            return pipe(prompt, \n",
        "                        init_image,\n",
        "                        mask_image=mask_image, \n",
        "                      #  strength=strength, \n",
        "                        guidance_scale=scale,\n",
        "                        num_inference_steps=steps,\n",
        "                        latents=latents).images\n",
        "\n",
        "         if pipe == upscale:\n",
        "            latents = get_latents(seed, height, width)\n",
        "            return pipe(prompt, \n",
        "                        init_image,\n",
        "                      #  mask_image=mask_image, \n",
        "                      #  strength=strength, \n",
        "                        guidance_scale=scale,\n",
        "                        num_inference_steps=steps,\n",
        "                        latents=latents).images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH74gBWDd2oq"
      },
      "outputs": [],
      "source": [
        "#@title Waiting for plugin requests\n",
        "\n",
        "import gc, math, os, pathlib, sys, time, random\n",
        "import json\n",
        "import base64\n",
        "import subprocess\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from torch import autocast\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from flask import Flask, Response, request, abort, make_response\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "from IPython import display\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageOps\n",
        "from types import SimpleNamespace\n",
        "\n",
        "prompts = []\n",
        "\n",
        "def DeforumArgs():\n",
        "    W = 0\n",
        "    H = 0\n",
        "\n",
        "    seed = -1\n",
        "    sampler = 'klms'\n",
        "    steps = 0\n",
        "    scale = 0\n",
        "    ddim_eta = 0.0\n",
        "    dynamic_threshold = None\n",
        "    static_threshold = None   \n",
        "    pipe = None\n",
        "\n",
        "    save_samples = True\n",
        "    save_settings = False\n",
        "    display_samples = False\n",
        "\n",
        "    n_batch = 1\n",
        "    batch_name = \"PLUGIN\"\n",
        "    filename_format = \"{timestring}_{index}_{prompt}.png\"\n",
        "    make_grid = False\n",
        "    grid_rows = 2 \n",
        "    outdir = get_output_folder(output_path, batch_name)\n",
        "\n",
        "    use_init = False\n",
        "    strength = 0\n",
        "    init_image = None\n",
        "    # Whiter areas of the mask are areas that change more\n",
        "    use_mask = False\n",
        "    invert_mask = False\n",
        "    mask_image = None\n",
        "\n",
        "    prompt = \"\"\n",
        "    timestring = \"\"\n",
        "\n",
        "    return locals()\n",
        "\n",
        "def sanitize(prompt):\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    tmp = ''.join(filter(whitelist.__contains__, prompt))\n",
        "    return tmp.replace(' ', '_')\n",
        "\n",
        "def get_output_folder(output_path, batch_folder):\n",
        "    out_path = os.path.join(output_path,time.strftime('%Y-%m'))\n",
        "    if batch_folder != \"\":\n",
        "        out_path = os.path.join(out_path, batch_folder)\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    return out_path\n",
        "\n",
        "def load_img(image, shape, use_alpha_as_mask=False, invert_mask=False):\n",
        "    # use_alpha_as_mask: Read the alpha channel of the image as the mask image\n",
        "    if isinstance(image, str):\n",
        "        path = image\n",
        "        if os.path.exists(path):\n",
        "           image = Image.open(path)\n",
        "        else:\n",
        "          return None, None\n",
        "\n",
        "    mask_image = None\n",
        "    if use_alpha_as_mask:\n",
        "        image = image.convert('RGBA')\n",
        "        red, green, blue, alpha = Image.Image.split(image)\n",
        "        mask_image = alpha.convert('L')\n",
        "        if not invert_mask:\n",
        "           mask_image = ImageOps.invert(mask_image)\n",
        "    else:\n",
        "        mask_image = Image.new(\"RGB\", (image.height, image.width))\n",
        "\n",
        "    image = image.convert('RGB')\n",
        "    image = image.resize(shape, resample=Image.LANCZOS)\n",
        "\n",
        "    return image, mask_image\n",
        "\n",
        "def render_image_batch(args):\n",
        "    args.prompts = {k: f\"{v:05d}\" for v, k in enumerate(prompts)}\n",
        "    \n",
        "    index = 0\n",
        "    \n",
        "    # function for init image batching\n",
        "    init_array = []\n",
        "    if args.use_init:\n",
        "        if not isinstance(args.init_image, str):\n",
        "            init_array = [args.init_image]\n",
        "        elif args.init_image == \"\":\n",
        "            raise FileNotFoundError(\"No path was given for init_image\")\n",
        "        elif args.init_image.startswith('http://') or args.init_image.startswith('https://'):\n",
        "            init_array.append(args.init_image)\n",
        "        elif not os.path.isfile(args.init_image):\n",
        "            if args.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n",
        "                args.init_image += \"/\" \n",
        "            for image in sorted(os.listdir(args.init_image)): # iterates dir and appends images to init_array\n",
        "                if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n",
        "                    init_array.append(args.init_image + image)\n",
        "        else:\n",
        "            init_array.append(args.init_image)\n",
        "    else:\n",
        "        init_array = [\"\"]\n",
        "\n",
        "    # when doing large batches don't flood browser with images\n",
        "    clear_between_batches = args.n_batch >= 32\n",
        "\n",
        "    for iprompt, prompt in enumerate(prompts):  \n",
        "        args.prompt = prompt\n",
        "        print(f\"Prompt {iprompt+1} of {len(prompts)}\")\n",
        "        print(f\"{args.prompt}\")\n",
        "\n",
        "        all_images = []\n",
        "\n",
        "        for batch_index in range(args.n_batch):\n",
        "            if clear_between_batches and batch_index % 32 == 0: \n",
        "                display.clear_output(wait=True)            \n",
        "            print(f\"Batch {batch_index+1} of {args.n_batch}\")\n",
        "            \n",
        "            for image in init_array: # iterates the init images\n",
        "                #The mask structure is white for inpainting and black for keeping as is\n",
        "                args.init_image, args.mask_image = load_img(image, \n",
        "                                                            shape=(args.W, args.H),  \n",
        "                                                            use_alpha_as_mask=args.use_mask,\n",
        "                                                            invert_mask=args.invert_mask)\n",
        "                \n",
        "                if args.save_samples:\n",
        "                   if args.init_image is not None:\n",
        "                      args.init_image.save(os.path.join(output_path, \"init.png\"))\n",
        "                   if args.mask_image is not None:\n",
        "                      args.mask_image.save(os.path.join(output_path, \"mask.png\"))\n",
        "\n",
        "                results = render_image(args.pipe, args.prompt, args.init_image, args.mask_image, args.strength, args.steps, args.scale, args.H, args.W, args.seed)       \n",
        "\n",
        "                for image in results:\n",
        "                    if args.make_grid:\n",
        "                        all_images.append(T.functional.pil_to_tensor(image))\n",
        "                    if args.save_samples:\n",
        "                        if args.filename_format == \"{timestring}_{index}_{prompt}.png\":\n",
        "                            filename = f\"{args.timestring}_{index:05}_{sanitize(prompt)[:160]}.png\"\n",
        "                        else:\n",
        "                            filename = f\"{args.timestring}_{index:05}_{args.seed}.png\"\n",
        "                        image.save(os.path.join(args.outdir, filename))\n",
        "                    if args.display_samples:\n",
        "                        display.display(image)\n",
        "                    index += 1\n",
        "\n",
        "        if args.make_grid:\n",
        "            grid = make_grid(all_images, nrow=int(len(all_images)/args.grid_rows))\n",
        "            grid = rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "            filename = f\"{args.timestring}_{iprompt:05d}_grid_{args.seed}.png\"\n",
        "            grid_image = Image.fromarray(grid.astype(np.uint8))\n",
        "            grid_image.save(os.path.join(args.outdir, filename))\n",
        "            display.clear_output(wait=True)            \n",
        "            display.display(grid_image)\n",
        "\n",
        "    return results\n",
        "\n",
        "args = SimpleNamespace(**DeforumArgs())\n",
        "args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "API_VERSION = 5\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/api/generate\", methods=[\"POST\"])\n",
        "def generateImages():\n",
        "    r = request\n",
        "    data = r.data.decode(\"utf-8\")\n",
        "    data = json.loads(data)\n",
        "\n",
        "    api_version = 0\n",
        "\n",
        "    if \"api_version\" in data:\n",
        "       api_version = int(data[\"api_version\"])\n",
        "\n",
        "    if api_version != API_VERSION:\n",
        "       abort(405)\n",
        "       \n",
        "    print(\"\\n\")\n",
        "    print(\"Parameters sent from Plugin\")\n",
        "    print(\"mode: \" + data[\"mode\"] + \", init_strength: \" + str(data[\"init_strength\"]) + \", prompt_strength: \" + str(data[\"prompt_strength\"]) + \", steps: \" + str(data[\"steps\"]) + \", width: \" + str(data[\"width\"]) + \", height: \" + str(data[\"height\"]) + \", prompt: \" + data[\"prompt\"] + \", seed: \" + str(data[\"seed\"]) + \", api_version: \" + str(data[\"api_version\"]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    args.W, args.H = map(lambda x: x - x % 64, (int(data[\"width\"]), int(data[\"height\"])))\n",
        "    args.strength = max(0.0, min(1.0, float(data[\"init_strength\"])))\n",
        "    args.scale = float(data[\"prompt_strength\"])\n",
        "    args.steps = int(data[\"steps\"])\n",
        "    args.use_init = False\n",
        "    args.use_mask = False\n",
        "    args.init_image = None\n",
        "    args.mask_image = None\n",
        "    args.pipe = None\n",
        "\n",
        "    init_img = \"\"\n",
        "    if data[\"mode\"] == \"MODE_IMG2IMG\" or data[\"mode\"] == \"MODE_INPAINTING\" or data[\"mode\"] == \"MODE_UPSCALING\":\n",
        "       img_data = base64.b64decode(data[\"init_img\"])\n",
        "       img_stream = BytesIO(img_data)\n",
        "       args.init_image = Image.open(img_stream)\n",
        "\n",
        "       args.use_init = True   \n",
        "       args.pipe = img2img\n",
        "\n",
        "       if data[\"mode\"] == \"MODE_UPSCALING\":\n",
        "          args.strength = 0.0\n",
        "          args.pipe = upscale\n",
        "\n",
        "       if data[\"mode\"] == \"MODE_INPAINTING\":\n",
        "          args.strength = 0.0\n",
        "          args.use_mask = True\n",
        "          args.pipe = inpaint\n",
        "    else:\n",
        "       args.pipe = text2img\n",
        "\n",
        "    if args.pipe is None:\n",
        "      abort(406)\n",
        "    \n",
        "    global prompts\n",
        "    prompts = [data[\"prompt\"]]\n",
        "\n",
        "    args.prompt = prompts\n",
        "    imgs_return = []\n",
        "\n",
        "    for counter in range(data[\"image_count\"]):\n",
        "       # clean up unused memory\n",
        "       gc.collect()\n",
        "       torch.cuda.empty_cache()\n",
        "    \n",
        "       args.seed = int(data[\"seed\"]) if int(data[\"seed\"]) != -1 else random.randint(0, 2**32)\n",
        "\n",
        "       print(\"Parameters used for generating\")\n",
        "       print(args)\n",
        "\n",
        "       img = render_image_batch(args)[0]\n",
        "\n",
        "       img_data = BytesIO()\n",
        "       img.save(img_data, format=\"PNG\")\n",
        "       img_data.seek(0)\n",
        "       img_encoded = base64.b64encode(img_data.read())\n",
        "       img_encoded = img_encoded.decode(\"utf-8\")\n",
        "\n",
        "       img_return = {\"seed\": args.seed, \"image\": img_encoded}\n",
        "       imgs_return.append(img_return)\n",
        "\n",
        "    data_return = {\"images\": imgs_return}\n",
        "    data_return = json.dumps(data_return)\n",
        "\n",
        "    if data[\"mode\"] == \"MODE_IMG2IMG\" or data[\"mode\"] == \"MODE_INPAINTING\":\n",
        "       if os.path.exists(init_img):\n",
        "          os.remove(init_img)\n",
        "\n",
        "    response = make_response()\n",
        "    response.headers[\"mimetype\"] = \"application/json\"\n",
        "    response.data = data_return\n",
        "    return response\n",
        "\n",
        "run_with_cloudflared(app)\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "0b4158c0de3566b734c6172715d060306cc14fb8cdd06f96dd34f7949c2579b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}